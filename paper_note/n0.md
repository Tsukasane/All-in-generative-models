### Reading Note for 
**A comprehensive survey and analysis of generative models in machine learning**

This paper mainly discussed the algos and implemenatations of different categories of generative models(GM).
* Gaussian Mixture Models ([GMM](#label_GMM))
* Hidden Markov Models ([HMM](#label_HMM))
* Latent Dirichlet Allocation (LDA)
* Boltzmann Machines
  - Restricted Boltzmann Machines (RBM)
  - Deep Belief Networks (DBN)
  - Deep Boltzmann Machines (DBM)
* Variational Autoencoders ([VAE](#label_VAE))
* Generative Adversarial Networks ([GANs](#label_GAN))

#### $\Rightarrow$ Discreminative models 
* draw the decision boundary in a data space;
* estimate P(Y|X) -- Y is the target.

#### $\Rightarrow$ Generative models
* learn the overall distribution of the data;
* estimate the distribution given by P(X|Y) and P(Y) -- X is the target

<a id="label_GMM"> </a>
**GMM** 

Different Gaussians are represented by different
{$\pi_i,\mu_i,\rho_i^2$} tuples, with parameters meaning weight, mean, and variance.
GMM is a mixture of them. Each Gaussian can be seen as a cluster, with:
$$
    \sum_{i=1}^n\pi_i = 1
$$

<div style="text-align:center">
    <img src="../figure/n0_fig0.jpg" alt="Fig.3 in the paper" width="300" height="200">
</div>

If we are given that  $z_k$(a latent that sampled from a Gaussian) is from a cluster $i$, the likelihood of observing $x_k$ is given as,
$$
    P(x_k| z_k=i, \mu_i,\rho_i) = N(x_k| \mu_i,\rho_i)
$$

Thus, for a N component Gaussian mixture model:
$$
    P(x| \{ \pi_k,\mu_k,\rho_k^2\}) = \sum_{k=1}^N w_ib(x | u_k, p_k)
$$

Which indicates the probability of the generated $x$ is sampled from each single cluster.

---

<a id="label_HMM"> </a>
**HMM**

A probabilistic representation of a system it models.
It is used for modeling linear problems involving time series or sequences.

The probability of reaching the next state is dependent on the transition function of the current state. Each state emits residues or symbols based on their symbol-emission probabilities. The Markov chain(state sequence) that led to these emissions are not observable.

---

<a id="label_VAE"></a>

**VAE**

* **Autoencoder**: unsupervised training with unlabelled data
    $$
       {\bf Z} = f({\bf WX}+b)
    $$
    $$
        L = ||x-\hat x||^2
    $$

    ${\bf Z}$ is a compressed important feature 
* **Variational Autoencoders**: generating data ${\bf x}$ from a unknown latent distribution $z$.
    
    Assume the data $\{x^i\}_{i=1}^N$ is generated by a true prior latent distribution $z$(a Gaussian) given by $p_{\theta}(z)$ where $\theta$ are the parameters of the model.

    The true condition is $p_{\theta}({\bf x}|z^i)$ but intractable.
    $$
        p_{\theta}({\bf x}) = \int p_{\theta}({\bf z})p_{\theta}({\bf x|z}) dz
    $$

    Because we don't know the $p_{\theta}({\bf x}) $, the posterior density is also intractable.
    $$
        p_{\theta}({\bf z|x}) = \frac{p_{\theta} {\bf (x|z)}p_{\theta}{\bf (z)}}{p_{\theta}{\bf (x)}} 
    $$
    

    However, we can <font color="Red"><b> approximate </b></font>  $p_{\theta}({\bf z|x})$ through an inference network $q_\phi({\bf z|x})$ which allows us to <font color="Red"><b>derive a tractable lower bound</b></font> which can be maximized by proper optimization.

    ![Fig14](../figure/n0_fig1.jpg)

    ${\bf (Left)}$ the inference network $q_\phi({\bf z|x})$   |   ${\bf (Right)}$ the generator network $p_{\theta}({\bf x|z})$. Their outputs are the means and the diagonal covariances.

    With $p_{\theta}(x^i)$ independent of $z$, the logarithm of the data likelihood can then be expressed as:


    $$

    \mathbb{E}_{{\bf z}\sim q_{\phi}({\bf z}|x^i)}[\operatorname{log}p_{\theta}(x^i)]  = \mathbb{E}{{\bf z}}\left[ \operatorname{log}\frac{p_{\theta}(x^i|{\bf z})p_{\theta}({\bf z})}{p_{\theta}({\bf z}|x^i)} \right] 
    
    $$
    
    $$

    =\quad \mathbb{E}{{\bf z}}\left[ \operatorname{log}\frac{p_{\theta}(x^i|{\bf z})p_{\theta}({\bf z})}{p_{\theta}({\bf z}|x^i)} \frac{q_\phi({\bf z|}x^i)}{q_\phi({\bf z|}x^i)} \right] 

    $$

    $$  
    =\quad \mathbb{E}_{\bf z}\left[  \operatorname{log}p_{\theta}(x^i|{\bf z})\right] 
    - \mathbb{E}_{\bf z}\left[  \operatorname{log}\frac{q_\phi({\bf z|}x^i)}{p_{\theta}({\bf z})}  \right] 
    + \mathbb{E}_{\bf z}\left[  \operatorname{log}\frac{q_\phi({\bf z|}x^i)}{p_{\theta}({\bf z|}x^i)}  \right] \\

    $$
    $$

    =\quad \mathbb{E}_{\bf z}\left[  \operatorname{log}p_{\theta}(x^i|{\bf z})\right] - KL(q_\phi({\bf z|}x^i) || p_{\theta}({\bf z})) + KL(q_\phi({\bf z|}x^i) || p_{\theta}({\bf z|}x^i))
    
    $$

    <font color="Red"><b> Concept:</b></font> [KL divergence](#label_KLdivergence)

    Then using <font color="Red"><b>term Ⅰ</b></font> and <font color="Red"><b>term Ⅱ</b></font> to get a tractable lower bound $\epsilon(x^i, \theta, \phi) \leq \operatorname{log}p_{\theta}{(x^i)}$ with its gradient available.

    * term Ⅰ $\Rightarrow$ uses reparameterization trick to make it differentiable & describes the reconstruction of input data.
    * term Ⅱ $\Rightarrow$ the posterior distribution must be as similar as possible to the prior.
    * term Ⅲ $\geq 0$

    Therefore, while training, we attempt to estimate the parameters $\theta'$ and $\phi'$ by maximizing $\epsilon(x^i, \theta, \phi)$ as

    $$
        \theta', \phi' = \operatorname{argmax}\limits_{\theta, \phi} \sum_{i=1}^N \epsilon(x^i, \theta, \phi)
    $$

<a id="label_GAN"></a>

--- 

**GAN**

* VAE: not the most accurate; blurriness in the generated images
* GAN: w/o the usual procedure of maximizing a log likelihood (<font color="Red"><b>term Ⅱ</b></font> in VAE); doesn't require Markov chains.

    * Deep Convolutional GANs (DCGANs),
    * Fully Connected and Convolutional GANs (FCC-GANs)
    * Conditional GANs (CGANs)
    * Stack GANs (SGAN)
    

### Important Concepts

<a id="label_KLdivergence"></a>

#### The Kullback-Leibler (KL) Divergence
is used to measure the similarity between two density functions $p(x)$ and $q(x)$, which is always $\geq0$ and is 0 iff $p=q$. It is defined by:
$$
    D(p || q) = \int p(x)\operatorname{log}\frac{p(x)}{q(x)}dx
$$

